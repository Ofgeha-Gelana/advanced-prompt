{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Loading fine-tuned model and the base model from Hugging Face, then perform a comparative analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TextClassificationPipeline\n",
    ")\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load Models --------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = \"distilbert-base-uncased\"\n",
    "finetuned_model = \"Ofge/finetuned_distilbert\"  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "base_tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(base_model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your fine-tuned model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_tokenizer = AutoTokenizer.from_pretrained(your_finetuned_model)\n",
    "finetuned_model = AutoModelForSequenceClassification.from_pretrained(your_finetuned_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create Pipelines with safety defaults\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "base_pipe = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=base_model_name,\n",
    "    device=device,\n",
    "    truncation=True,  # Add these defaults\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "finetuned_pipe = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=your_finetuned_model,\n",
    "    device=device,\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Evaluation Function ------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(pipeline, test_texts, test_labels):\n",
    "    predictions = pipeline(test_texts.tolist())\n",
    "    pred_labels = [1 if pred['label'] == 'POSITIVE' else 0 for pred in predictions]\n",
    "    return classification_report(test_labels, pred_labels, output_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Load Test Data -----------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"../data/IMDB Dataset.csv\")\n",
    "test_df = test_df.sample(10, random_state=42)  # Smaller subset for demo\n",
    "test_df['sentiment'] = test_df['sentiment'].map({'positive':1, 'negative':0})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Run Comparisons ----------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ofge/advanced-prompt/adv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ofge/advanced-prompt/adv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ofge/advanced-prompt/adv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ofge/advanced-prompt/adv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ofge/advanced-prompt/adv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ofge/advanced-prompt/adv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "base_results = evaluate(base_pipe, test_df['review'], test_df['sentiment'])\n",
    "finetuned_results = evaluate(finetuned_pipe, test_df['review'], test_df['sentiment'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Comparative Analysis ----------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Performance Comparison ===\n",
      "                  Accuracy  F1-Score  Precision  Recall\n",
      "Base Model             0.4  0.228571       0.16     0.4\n",
      "Fine-tuned Model       0.4  0.228571       0.16     0.4\n"
     ]
    }
   ],
   "source": [
    "comparison = pd.DataFrame({\n",
    "    'Base Model': {\n",
    "        'Accuracy': base_results['accuracy'],\n",
    "        'F1-Score': base_results['weighted avg']['f1-score'],\n",
    "        'Precision': base_results['weighted avg']['precision'],\n",
    "        'Recall': base_results['weighted avg']['recall']\n",
    "    },\n",
    "    'Fine-tuned Model': {\n",
    "        'Accuracy': finetuned_results['accuracy'],\n",
    "        'F1-Score': finetuned_results['weighted avg']['f1-score'],\n",
    "        'Precision': finetuned_results['weighted avg']['precision'],\n",
    "        'Recall': finetuned_results['weighted avg']['recall']\n",
    "    }\n",
    "}).T\n",
    "\n",
    "print(\"=== Performance Comparison ===\")\n",
    "print(comparison)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Example Predictions ------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample Predictions ===\n",
      "\n",
      "Text: This movie was absolutely wonderful!\n",
      "Base: LABEL_0 (0.57)\n",
      "Fine-tuned: LABEL_1 (0.52)\n",
      "\n",
      "Text: The plot was terrible and boring.\n",
      "Base: LABEL_0 (0.55)\n",
      "Fine-tuned: LABEL_1 (0.53)\n",
      "\n",
      "Text: It was okay, nothing special.\n",
      "Base: LABEL_0 (0.57)\n",
      "Fine-tuned: LABEL_1 (0.52)\n"
     ]
    }
   ],
   "source": [
    "sample_texts = [\n",
    "    \"This movie was absolutely wonderful!\",\n",
    "    \"The plot was terrible and boring.\",\n",
    "    \"It was okay, nothing special.\"\n",
    "]\n",
    "\n",
    "print(\"\\n=== Sample Predictions ===\")\n",
    "for text in sample_texts:\n",
    "    base_pred = base_pipe(text)[0]\n",
    "    finetuned_pred = finetuned_pipe(text)[0]\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"Base: {base_pred['label']} ({base_pred['score']:.2f})\")\n",
    "    print(f\"Fine-tuned: {finetuned_pred['label']} ({finetuned_pred['score']:.2f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Advanced Analysis -------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Prediction Disagreements ===\n",
      "                                                text      true base_pred  \\\n",
      "0  I really liked this Summerslam due to the look...  POSITIVE   LABEL_0   \n",
      "1  Not many television shows appeal to quite as m...  POSITIVE   LABEL_0   \n",
      "2  The film quickly gets to a major chase scene w...  NEGATIVE   LABEL_0   \n",
      "3  Jane Austen would definitely approve of this o...  POSITIVE   LABEL_0   \n",
      "4  Expectations were somewhat high for me when I ...  NEGATIVE   LABEL_0   \n",
      "\n",
      "   base_score  ft_pred  ft_score  agreement  \n",
      "0    0.542781  LABEL_1  0.550927      False  \n",
      "1    0.535357  LABEL_1  0.517670      False  \n",
      "2    0.536125  LABEL_1  0.531626      False  \n",
      "3    0.544320  LABEL_1  0.527887      False  \n",
      "4    0.526478  LABEL_1  0.537122      False  \n"
     ]
    }
   ],
   "source": [
    "def compare_predictions(texts, true_labels):\n",
    "    results = []\n",
    "    for text, label in zip(texts, true_labels):\n",
    "        base_pred = base_pipe(text)[0]\n",
    "        ft_pred = finetuned_pipe(text)[0]\n",
    "        results.append({\n",
    "            'text': text[:50] + \"...\",\n",
    "            'true': 'POSITIVE' if label == 1 else 'NEGATIVE',\n",
    "            'base_pred': base_pred['label'],\n",
    "            'base_score': base_pred['score'],\n",
    "            'ft_pred': ft_pred['label'],\n",
    "            'ft_score': ft_pred['score'],\n",
    "            'agreement': base_pred['label'] == ft_pred['label']\n",
    "        })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "disagreements = compare_predictions(test_df['review'], test_df['sentiment'])\n",
    "print(\"\\n=== Prediction Disagreements ===\")\n",
    "print(disagreements[~disagreements['agreement']].head())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMhKXGux5Thga4PcNmclG9H",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "adv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
