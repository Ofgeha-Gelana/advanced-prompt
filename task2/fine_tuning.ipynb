{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c67bd3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available labels: Counter({47: 3167, 65: 2493, 26: 2427, 38: 2340, 79: 1808, 85: 1469, 2: 1467, 7: 1327, 41: 1224, 89: 1166, 88: 1112, 51: 1104, 87: 1089, 58: 1034, 20: 1012, 45: 1009, 46: 976, 19: 960, 49: 936, 97: 878, 92: 866, 68: 856, 96: 768, 61: 741, 43: 677, 75: 664, 11: 656, 74: 621, 10: 620, 31: 590, 13: 588, 48: 587, 42: 578, 76: 569, 29: 566, 83: 563, 9: 549, 18: 541, 99: 517, 98: 499, 59: 479, 53: 473, 24: 469, 33: 462, 67: 459, 95: 451, 63: 445, 84: 444, 22: 444, 16: 434, 80: 419, 66: 409, 23: 406, 12: 397, 15: 389, 55: 386, 6: 385, 86: 380, 54: 366, 64: 363, 28: 354, 32: 354, 91: 353, 4: 352, 17: 349, 71: 345, 93: 341, 62: 327, 77: 327, 81: 326, 0: 326, 39: 324, 73: 323, 90: 317, 57: 313, 35: 308, 52: 304, 44: 300, 27: 278, 50: 271, 30: 263, 40: 258, 60: 255, 1: 252, 36: 244, 82: 238, 34: 229, 56: 202, 37: 201, 69: 190, 5: 178, 21: 175, 25: 168, 94: 131, 70: 125, 78: 118, 3: 106, 72: 47, 8: 31, 14: 23})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"lex_glue\", \"ledgar\")[\"train\"]  # LEDGAR only has train split\n",
    "\n",
    "# Inspect label distribution\n",
    "from collections import Counter\n",
    "label_counts = Counter(dataset[\"label\"])\n",
    "print(\"Available labels:\", label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a1b84f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 60000/60000 [00:00<00:00, 274453.83 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset size: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Pick two common labels that exist in the dataset\n",
    "LABEL1, LABEL2 = \"GOVERNING LAW\", \"AMENDMENT\"  # Adjust based on your inspection\n",
    "\n",
    "# Filter and binarize\n",
    "filtered = dataset.filter(lambda x: x[\"label\"] in [LABEL1, LABEL2])\n",
    "filtered = filtered.map(lambda x: {\n",
    "    \"text\": x[\"text\"],\n",
    "    \"label\": 0 if x[\"label\"] == LABEL1 else 1  # Convert to binary\n",
    "})\n",
    "\n",
    "print(f\"Filtered dataset size: {len(filtered)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cedfcf8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using synthetic data as fallback\n"
     ]
    }
   ],
   "source": [
    "# If filtered is empty, use this synthetic dataset\n",
    "import pandas as pd\n",
    "from datasets import Dataset  # Import Dataset class\n",
    "\n",
    "if len(filtered) == 0:\n",
    "    print(\"Using synthetic data as fallback\")\n",
    "    data = {\n",
    "        \"text\": [\n",
    "            \"This Agreement shall be governed by New York law\",  # 0\n",
    "            \"Party A shall indemnify Party B for all losses\",     # 1\n",
    "            \"The term 'Confidential Information' means...\",       # 0\n",
    "            \"Vendor must deliver goods by January 1\"             # 1\n",
    "        ],\n",
    "        \"label\": [0, 1, 0, 1]\n",
    "    }\n",
    "    filtered = Dataset.from_pandas(pd.DataFrame(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748c00f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "# Split dataset\n",
    "split = filtered.train_test_split(test_size=0.2)\n",
    "\n",
    "# Tokenization (CPU-optimized)\n",
    "# from transformers import DistilBertTokenizer\n",
    "# tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "from transformers import DistilBertForSequenceClassification\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "\n",
    "tokenized = split.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b6c079",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3/3 [00:00<00:00, 648.50 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 256.58 examples/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Trainer, TrainingArguments\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m training_args = \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./results\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Reduced for CPU\u001b[39;49;00m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;66;43;03m# Fewer epochs for CPU\u001b[39;49;00m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mepoch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreport_to\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnone\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# Disable MLflow logging\u001b[39;49;00m\n\u001b[32m     29\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m trainer = Trainer(\n\u001b[32m     32\u001b[39m     model=model,\n\u001b[32m     33\u001b[39m     args=training_args,\n\u001b[32m     34\u001b[39m     train_dataset=tokenized[\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     35\u001b[39m     eval_dataset=tokenized[\u001b[33m\"\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     36\u001b[39m )\n\u001b[32m     37\u001b[39m trainer.train()\n",
      "\u001b[31mTypeError\u001b[39m: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Training\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=4,  # Reduced for CPU\n",
    "    num_train_epochs=2,             # Fewer epochs for CPU\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\"                # Disable MLflow logging\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"test\"],\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a050c1cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb71d68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b04a6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24b195a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e1b1a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bad9110",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e647e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb85162d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load LEDGAR dataset correctly\n",
    "dataset = load_dataset(\"lex_glue\", \"ledgar\")\n",
    "\n",
    "# Access the train split (LEDGAR only has train split)\n",
    "train_dataset = dataset[\"train\"]\n",
    "\n",
    "# Shuffle and select subset properly\n",
    "subset = train_dataset.shuffle(seed=42).select(range(2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc23e763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Adjustments', 'Agreements', 'Amendments', 'Anti-Corruption Laws', 'Applicable Laws', 'Approvals', 'Arbitration', 'Assignments', 'Assigns', 'Authority', 'Authorizations', 'Base Salary', 'Benefits', 'Binding Effects', 'Books', 'Brokers', 'Capitalization', 'Change In Control', 'Closings', 'Compliance With Laws', 'Confidentiality', 'Consent To Jurisdiction', 'Consents', 'Construction', 'Cooperation', 'Costs', 'Counterparts', 'Death', 'Defined Terms', 'Definitions', 'Disability', 'Disclosures', 'Duties', 'Effective Dates', 'Effectiveness', 'Employment', 'Enforceability', 'Enforcements', 'Entire Agreements', 'Erisa', 'Existence', 'Expenses', 'Fees', 'Financial Statements', 'Forfeitures', 'Further Assurances', 'General', 'Governing Laws', 'Headings', 'Indemnifications', 'Indemnity', 'Insurances', 'Integration', 'Intellectual Property', 'Interests', 'Interpretations', 'Jurisdictions', 'Liens', 'Litigations', 'Miscellaneous', 'Modifications', 'No Conflicts', 'No Defaults', 'No Waivers', 'Non-Disparagement', 'Notices', 'Organizations', 'Participations', 'Payments', 'Positions', 'Powers', 'Publicity', 'Qualifications', 'Records', 'Releases', 'Remedies', 'Representations', 'Sales', 'Sanctions', 'Severability', 'Solvency', 'Specific Performance', 'Submission To Jurisdiction', 'Subsidiaries', 'Successors', 'Survival', 'Tax Withholdings', 'Taxes', 'Terminations', 'Terms', 'Titles', 'Transactions With Affiliates', 'Use Of Proceeds', 'Vacations', 'Venues', 'Vesting', 'Waiver Of Jury Trials', 'Waivers', 'Warranties', 'Withholdings']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 2000/2000 [00:00<00:00, 33414.49 examples/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Invalid key: 0 is out of bounds for size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     15\u001b[39m filtered_dataset = subset.filter(\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mObligation\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDefinition\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     17\u001b[39m ).map(map_label)\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Verify\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mfiltered_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m)  \u001b[38;5;66;03m# Should show text with 0 or 1 label\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/advanced-prompt/adv/lib/python3.12/site-packages/datasets/arrow_dataset.py:2777\u001b[39m, in \u001b[36mDataset.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   2775\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[32m   2776\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2777\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/advanced-prompt/adv/lib/python3.12/site-packages/datasets/arrow_dataset.py:2761\u001b[39m, in \u001b[36mDataset._getitem\u001b[39m\u001b[34m(self, key, **kwargs)\u001b[39m\n\u001b[32m   2759\u001b[39m format_kwargs = format_kwargs \u001b[38;5;28;01mif\u001b[39;00m format_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m   2760\u001b[39m formatter = get_formatter(format_type, features=\u001b[38;5;28mself\u001b[39m._info.features, **format_kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m2761\u001b[39m pa_subtable = \u001b[43mquery_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2762\u001b[39m formatted_output = format_table(\n\u001b[32m   2763\u001b[39m     pa_subtable, key, formatter=formatter, format_columns=format_columns, output_all_columns=output_all_columns\n\u001b[32m   2764\u001b[39m )\n\u001b[32m   2765\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/advanced-prompt/adv/lib/python3.12/site-packages/datasets/formatting/formatting.py:607\u001b[39m, in \u001b[36mquery_table\u001b[39m\u001b[34m(table, key, indices)\u001b[39m\n\u001b[32m    605\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    606\u001b[39m     size = indices.num_rows \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m table.num_rows\n\u001b[32m--> \u001b[39m\u001b[32m607\u001b[39m     \u001b[43m_check_valid_index_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    608\u001b[39m \u001b[38;5;66;03m# Query the main table\u001b[39;00m\n\u001b[32m    609\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/advanced-prompt/adv/lib/python3.12/site-packages/datasets/formatting/formatting.py:547\u001b[39m, in \u001b[36m_check_valid_index_key\u001b[39m\u001b[34m(key, size)\u001b[39m\n\u001b[32m    545\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mint\u001b[39m):\n\u001b[32m    546\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (key < \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m key + size < \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (key >= size):\n\u001b[32m--> \u001b[39m\u001b[32m547\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid key: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is out of bounds for size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    549\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mslice\u001b[39m):\n",
      "\u001b[31mIndexError\u001b[39m: Invalid key: 0 is out of bounds for size 0"
     ]
    }
   ],
   "source": [
    "# Get the actual label names from the dataset\n",
    "print(subset.features[\"label\"].names)  # Check available labels\n",
    "\n",
    "# We'll use these two common classes for binary classification\n",
    "label_map = {\n",
    "    \"Obligation\": 1,    # High risk\n",
    "    \"Definition\": 0     # Low risk\n",
    "}\n",
    "\n",
    "# Filter and map labels\n",
    "def map_label(example):\n",
    "    return {\"label\": label_map.get(example[\"label\"]), \n",
    "            \"text\": example[\"text\"]}\n",
    "\n",
    "filtered_dataset = subset.filter(\n",
    "    lambda x: x[\"label\"] in [\"Obligation\", \"Definition\"]\n",
    ").map(map_label)\n",
    "\n",
    "# Verify\n",
    "print(filtered_dataset[0])  # Should show text with 0 or 1 label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ac1e878",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "BuilderConfig 'contract_analysis' not found. Available: ['case_hold', 'ecthr_a', 'ecthr_b', 'eurlex', 'ledgar', 'scotus', 'unfair_tos']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m dataset = \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlex_glue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontract_analysis\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 5,000 labeled clauses\u001b[39;00m\n\u001b[32m      4\u001b[39m dataset = dataset.shuffle(seed=\u001b[32m42\u001b[39m).select(\u001b[38;5;28mrange\u001b[39m(\u001b[32m1000\u001b[39m))  \u001b[38;5;66;03m# Smaller subset for demo\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/advanced-prompt/adv/lib/python3.12/site-packages/datasets/load.py:2062\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[39m\n\u001b[32m   2057\u001b[39m verification_mode = VerificationMode(\n\u001b[32m   2058\u001b[39m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode.BASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode.ALL_CHECKS\n\u001b[32m   2059\u001b[39m )\n\u001b[32m   2061\u001b[39m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2062\u001b[39m builder_instance = \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2063\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2064\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2065\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2066\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2067\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2068\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2069\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2073\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2074\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2075\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2076\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2077\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2079\u001b[39m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[32m   2080\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/advanced-prompt/adv/lib/python3.12/site-packages/datasets/load.py:1819\u001b[39m, in \u001b[36mload_dataset_builder\u001b[39m\u001b[34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[39m\n\u001b[32m   1817\u001b[39m builder_cls = get_dataset_builder_class(dataset_module, dataset_name=dataset_name)\n\u001b[32m   1818\u001b[39m \u001b[38;5;66;03m# Instantiate the dataset builder\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1819\u001b[39m builder_instance: DatasetBuilder = \u001b[43mbuilder_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1820\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1821\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1822\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1823\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1824\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1825\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mhash\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mdataset_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1826\u001b[39m \u001b[43m    \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1827\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1828\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1829\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1830\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mbuilder_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1832\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1833\u001b[39m builder_instance._use_legacy_cache_dir_if_possible(dataset_module)\n\u001b[32m   1835\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/advanced-prompt/adv/lib/python3.12/site-packages/datasets/builder.py:343\u001b[39m, in \u001b[36mDatasetBuilder.__init__\u001b[39m\u001b[34m(self, cache_dir, dataset_name, config_name, hash, base_path, info, features, token, repo_id, data_files, data_dir, storage_options, writer_batch_size, **config_kwargs)\u001b[39m\n\u001b[32m    341\u001b[39m     config_kwargs[\u001b[33m\"\u001b[39m\u001b[33mdata_dir\u001b[39m\u001b[33m\"\u001b[39m] = data_dir\n\u001b[32m    342\u001b[39m \u001b[38;5;28mself\u001b[39m.config_kwargs = config_kwargs\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28mself\u001b[39m.config, \u001b[38;5;28mself\u001b[39m.config_id = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_builder_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[38;5;66;03m# prepare info: DatasetInfo are a standardized dataclass across all datasets\u001b[39;00m\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# Prefill datasetinfo\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    352\u001b[39m     \u001b[38;5;66;03m# TODO FOR PACKAGED MODULES IT IMPORTS DATA FROM src/packaged_modules which doesn't make sense\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/advanced-prompt/adv/lib/python3.12/site-packages/datasets/builder.py:570\u001b[39m, in \u001b[36mDatasetBuilder._create_builder_config\u001b[39m\u001b[34m(self, config_name, custom_features, **config_kwargs)\u001b[39m\n\u001b[32m    568\u001b[39m     builder_config = \u001b[38;5;28mself\u001b[39m.builder_configs.get(config_name)\n\u001b[32m    569\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m builder_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.BUILDER_CONFIGS:\n\u001b[32m--> \u001b[39m\u001b[32m570\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    571\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBuilderConfig \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m not found. Available: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m.builder_configs.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    572\u001b[39m         )\n\u001b[32m    574\u001b[39m \u001b[38;5;66;03m# if not using an existing config, then create a new config on the fly\u001b[39;00m\n\u001b[32m    575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m builder_config:\n",
      "\u001b[31mValueError\u001b[39m: BuilderConfig 'contract_analysis' not found. Available: ['case_hold', 'ecthr_a', 'ecthr_b', 'eurlex', 'ledgar', 'scotus', 'unfair_tos']"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"lex_glue\", \"contract_analysis\")  # 5,000 labeled clauses\n",
    "dataset = dataset.shuffle(seed=42).select(range(1000))  # Smaller subset for demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855c3150",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
