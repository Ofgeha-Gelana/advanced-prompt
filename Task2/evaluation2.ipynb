{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79b4b946",
   "metadata": {},
   "source": [
    "#### Evaluating with Advanced prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1cfa3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ofge/advanced-prompt/adv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification\n",
    ")\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2cf5d5",
   "metadata": {},
   "source": [
    "#### 1. Load Models --------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68dd5395",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 1. Load Models --------------------------------------------------\n",
    "base_model_name = \"distilbert-base-uncased\"\n",
    "your_finetuned_model = \"Ofge/finetuned_distilbert\"  # Replace with your actual fine-tuned model path\n",
    "\n",
    "# Load models and tokenizers\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(base_model_name)\n",
    "\n",
    "finetuned_tokenizer = AutoTokenizer.from_pretrained(your_finetuned_model)\n",
    "finetuned_model = AutoModelForSequenceClassification.from_pretrained(your_finetuned_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8201e4f",
   "metadata": {},
   "source": [
    "#### 2. Define Prompt Templates ---------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df840d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ZERO_SHOT_TEMPLATE = \"\"\"\n",
    "Analyze the sentiment of this movie review:\n",
    "Review: {review}\n",
    "Sentiment: [POSITIVE/NEGATIVE]\n",
    "\"\"\"\n",
    "\n",
    "FEW_SHOT_TEMPLATE = \"\"\"\n",
    "Classify these movie review examples:\n",
    "1. \"This film was amazing!\" → POSITIVE\n",
    "2. \"I hated every minute.\" → NEGATIVE\n",
    "3. \"The acting was terrible.\" → NEGATIVE\n",
    "4. \"A masterpiece of cinema.\" → POSITIVE\n",
    "5. \"{review}\" → \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005fcc7e",
   "metadata": {},
   "source": [
    "####  3. Evaluation Function with Prompting ----------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5eebce12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_prompts(model, tokenizer, test_texts, test_labels, prompt_type=\"zero\"):\n",
    "    predictions = []\n",
    "    for text in test_texts:\n",
    "        try:\n",
    "            if prompt_type == \"zero\":\n",
    "                prompted_text = ZERO_SHOT_TEMPLATE.format(review=text)\n",
    "            else:\n",
    "                prompted_text = FEW_SHOT_TEMPLATE.format(review=text[:500])\n",
    "            \n",
    "            inputs = tokenizer(prompted_text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                pred = torch.argmax(outputs.logits).item()\n",
    "            \n",
    "            predictions.append(pred)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing text: {e}\")\n",
    "            predictions.append(0)\n",
    "    \n",
    "    # Generate evaluation report\n",
    "    report = classification_report(test_labels, predictions, output_dict=True)\n",
    "    return {\n",
    "        'accuracy': report['accuracy'],\n",
    "        'f1': report['weighted avg']['f1-score'],\n",
    "        'precision': report['weighted avg']['precision'],\n",
    "        'recall': report['weighted avg']['recall']\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ab815e",
   "metadata": {},
   "source": [
    "####  4. Load Test Data -----------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5cedd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"../data/IMDB Dataset.csv\")\n",
    "test_df = test_df.sample(200, random_state=42)  # Use subset for speed\n",
    "test_df['sentiment'] = test_df['sentiment'].map({'positive': 1, 'negative': 0})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f539634d",
   "metadata": {},
   "source": [
    "#### 5. Run All Comparisons ------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb414471",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ofge/advanced-prompt/adv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ofge/advanced-prompt/adv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ofge/advanced-prompt/adv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ofge/advanced-prompt/adv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ofge/advanced-prompt/adv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ofge/advanced-prompt/adv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ofge/advanced-prompt/adv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ofge/advanced-prompt/adv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ofge/advanced-prompt/adv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "# Base Model Evaluations\n",
    "results['base_zero'] = evaluate_with_prompts(base_model, base_tokenizer, \n",
    "                                             test_df['review'], test_df['sentiment'], \"zero\")\n",
    "\n",
    "results['base_few'] = evaluate_with_prompts(base_model, base_tokenizer, \n",
    "                                            test_df['review'], test_df['sentiment'], \"few\")\n",
    "\n",
    "# Fine-Tuned Model Evaluations\n",
    "results['ft_zero'] = evaluate_with_prompts(finetuned_model, finetuned_tokenizer, \n",
    "                                           test_df['review'], test_df['sentiment'], \"zero\")\n",
    "\n",
    "results['ft_few'] = evaluate_with_prompts(finetuned_model, finetuned_tokenizer, \n",
    "                                          test_df['review'], test_df['sentiment'], \"few\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099ad80c",
   "metadata": {},
   "source": [
    "#### 6. Comparative Analysis ----------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d81c55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Comprehensive Performance Comparison ===\n",
      "           Base Zero-shot  Base Few-shot  FT Zero-shot  FT Few-shot\n",
      "Accuracy            0.520          0.525         0.525        0.525\n",
      "F1-Score            0.359          0.361         0.361        0.361\n",
      "Precision           0.274          0.276         0.276        0.276\n",
      "Recall              0.520          0.525         0.525        0.525\n"
     ]
    }
   ],
   "source": [
    "metrics = ['accuracy', 'f1', 'precision', 'recall']\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Base Zero-shot': [results['base_zero'][m] for m in metrics],\n",
    "    'Base Few-shot': [results['base_few'][m] for m in metrics],\n",
    "    'FT Zero-shot': [results['ft_zero'][m] for m in metrics],\n",
    "    'FT Few-shot': [results['ft_few'][m] for m in metrics]\n",
    "}, index=['Accuracy', 'F1-Score', 'Precision', 'Recall'])\n",
    "\n",
    "print(\"=== Comprehensive Performance Comparison ===\")\n",
    "print(comparison.round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2542d20",
   "metadata": {},
   "source": [
    "#### 7. Example Predictions ------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fead9cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Prediction Examples ===\n",
      "\n",
      "Review: The cinematography was stunning but the plot was weak...\n",
      "Base Zero: POS\n",
      "Base Few: POS\n",
      "FT Zero: POS\n",
      "FT Few: POS\n",
      "\n",
      "Review: Absolutely the worst movie I've seen this year...\n",
      "Base Zero: POS\n",
      "Base Few: POS\n",
      "FT Zero: POS\n",
      "FT Few: POS\n",
      "\n",
      "Review: A perfect blend of humor and heart...\n",
      "Base Zero: POS\n",
      "Base Few: POS\n",
      "FT Zero: POS\n",
      "FT Few: POS\n"
     ]
    }
   ],
   "source": [
    "sample_texts = [\n",
    "    \"The cinematography was stunning but the plot was weak\",\n",
    "    \"Absolutely the worst movie I've seen this year\",\n",
    "    \"A perfect blend of humor and heart\"\n",
    "]\n",
    "\n",
    "print(\"\\n=== Prediction Examples ===\")\n",
    "for text in sample_texts:\n",
    "    print(f\"\\nReview: {text[:100]}...\")\n",
    "\n",
    "    def simple_predict(model, tokenizer, text, prompt_type):\n",
    "        prompted = ZERO_SHOT_TEMPLATE.format(review=text) if prompt_type == \"zero\" else FEW_SHOT_TEMPLATE.format(review=text)\n",
    "        inputs = tokenizer(prompted, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "        return \"POS\" if torch.argmax(logits).item() == 1 else \"NEG\"\n",
    "\n",
    "    print(f\"Base Zero: {simple_predict(base_model, base_tokenizer, text, 'zero')}\")\n",
    "    print(f\"Base Few: {simple_predict(base_model, base_tokenizer, text, 'few')}\")\n",
    "    print(f\"FT Zero: {simple_predict(finetuned_model, finetuned_tokenizer, text, 'zero')}\")\n",
    "    print(f\"FT Few: {simple_predict(finetuned_model, finetuned_tokenizer, text, 'few')}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
